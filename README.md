# AIOS Demo for Windows

This project implements a research-grade, Python-based AIOS (Agentic Interactive Operating System) demo for Windows. It follows a layered, replayable architecture, integrating various components to enable an AI agent to perceive, reason, and act on the operating system.

## System Architecture

The AIOS architecture is layered, designed for modularity, replayability, and learning. It consists of the following key layers:

*   **Observers**: Capture raw signals from the environment (e.g., screenshots, UI Automation trees).
*   **Protocol1**: Processes raw signals into structured `ObservationEvent`s, leveraging a Protocol LLM for perception structuring. It also updates the `GraphMemory`.
*   **Agent**: Makes decisions based on `ObservationEvent`s, `GraphMemory` summaries, and user instructions, using a Core Agent LLM as its decision engine.
*   **Protocol2**: Validates and compiles the `ActionPlan` generated by the Agent into `VerifiedActionPlan`s.
*   **Actuator**: Executes the actions defined in `VerifiedActionPlan`s on the operating system (e.g., keyboard presses, mouse clicks, typing).

All inter-layer communication uses versioned Pydantic schemas, and the entire process is replayable via an append-only JSONL event stream (`events.jsonl`).

## Run Instructions

To run the AIOS Demo:

1.  **Activate Virtual Environment**:
    ```bash
    .\.venv\Scripts\activate
    ```
    (On Linux/macOS, it might be `source ./.venv/bin/activate`)

2.  **Launch the GUI**:
    ```bash
    python gui.py
    ```

3.  **Input LLM API Key**: Enter your Gemini API Key in the designated field in the GUI. (You can obtain one from [Google AI Studio](https://aistudio.google.com/app/apikey)).

4.  **Type Instruction**: Enter your desired instruction for the AIOS agent in the "User Instruction" text area.

5.  **Run**: Click the "Run AIOS Demo" button.

## Supported Commands

The AIOS agent, powered by the integrated real LLMs, can interpret and execute various instructions. Here are some examples:

*   **"Open Notepad and type Hello AIOS"**:
    *   (Requires Notepad to be manually opened and focused beforehand for current `Actuator` capabilities).
    *   The agent will detect Notepad and type the specified text.
*   **"Play Chrome Dino"**:
    *   (Requires `chrome://dino` to be open and focused in Chrome beforehand).
    *   The agent will press the "space" key to make the dinosaur jump.
*   **"Press space"**:
    *   The agent will perform a spacebar key press.
*   **"Type hello world"**:
    *   The agent will type "hello world" into the currently focused window.
*   **"Switch to Chrome"**:
    *   The agent will log its intention to switch to Chrome, as a direct "switch window" action is not yet implemented in the Actuator.

## Replay Instructions

The AIOS demo generates an `events.jsonl` file in the `aios_demo_runs/<timestamp>` directory for each run. This file contains a chronological log of all `ObservationEvent`s, `ActionPlan`s, `Receipt`s, and `GraphUpdate`s.

To replay a previous run (future implementation detail, current system logs but doesn't explicitly replay):

1.  Locate the `events.jsonl` file for the run you wish to replay.
2.  (Future: A dedicated replay script or mode will consume this JSONL file to reconstruct the state and observe the agent's decisions and actions without requiring live interaction).

## Project Structure

```
.
├── aios_demo.py            # Main entry point for running a single AIOS cycle
├── gui.py                  # Tkinter GUI for user interaction
├── README.md               # This file
├── ...
└── aios/
    ├── agent/
    │   └── main_agent.py   # Agent's decision-making logic
    ├── actuators/
    │   └── main_actuator.py# Executes actions on the OS
    ├── llm/
    │   └── llm_client.py   # Client for interacting with real LLM APIs (Gemini)
    ├── memory/
    │   └── graph.py        # Graph-based memory for the agent
    ├── observers/
    │   ├── screenshot.py   # Captures screenshots
    │   └── uia.py          # Captures UI Automation trees
    ├── protocols/
    │   ├── action_protocol.py # Protocol2 logic
    │   ├── llm_connector.py # Connects to LLM, handles prompts
    │   └── schema.py       # Pydantic data schemas
    └── prompts/
        ├── core_llm_prompt.txt     # System prompt for Core Agent LLM
        └── protocol_llm_prompt.txt # System prompt for Protocol LLM
```